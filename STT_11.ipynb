{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Dataset Preparation"
      ],
      "metadata": {
        "id": "zLtmVQSuixfW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2Pvcj7w9Tj0I"
      },
      "outputs": [],
      "source": [
        "#import all the necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import copy\n",
        "import os\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the training dataset and test data.\n"
      ],
      "metadata": {
        "id": "OoDyR9WZi5yl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the training and test datasets (no headers)\n",
        "train_df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/train.tsv\",\n",
        "    sep='\\t',\n",
        "    header=None,\n",
        "    names=[\"sentence\", \"label\"]\n",
        ")\n",
        "\n",
        "test_df = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/clairett/pytorch-sentiment-classification/master/data/SST2/test.tsv\",\n",
        "    sep='\\t',\n",
        "    header=None,\n",
        "    names=[\"sentence\", \"label\"]\n",
        ")"
      ],
      "metadata": {
        "id": "wlMAl0GoTu3c"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "_-6fCQnChECL",
        "outputId": "1855f5a7-5720-4458-ba82-113bacd5fd89"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  label\n",
              "0  a stirring , funny and finally transporting re...      1\n",
              "1  apparently reassembled from the cutting room f...      0\n",
              "2  they presume their audience wo n't sit still f...      0\n",
              "3  this is a visually stunning rumination on love...      1\n",
              "4  jonathan parker 's bartleby should have been t...      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-66c8f360-fb15-4cee-a824-335db636d6da\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>a stirring , funny and finally transporting re...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>apparently reassembled from the cutting room f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>they presume their audience wo n't sit still f...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>this is a visually stunning rumination on love...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jonathan parker 's bartleby should have been t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-66c8f360-fb15-4cee-a824-335db636d6da')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-66c8f360-fb15-4cee-a824-335db636d6da button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-66c8f360-fb15-4cee-a824-335db636d6da');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-a5619d10-7a17-4b6b-8630-5d5f58fc9eca\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-a5619d10-7a17-4b6b-8630-5d5f58fc9eca')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-a5619d10-7a17-4b6b-8630-5d5f58fc9eca button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df",
              "summary": "{\n  \"name\": \"train_df\",\n  \"rows\": 6920,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 6911,\n        \"samples\": [\n          \"a loud , brash and mainly unfunny high school comedy\",\n          \"the real star of this movie is the score , as in the songs translate well to film , and it 's really well directed\",\n          \"rosenthal lrb halloween ii rrb seems to have forgotten everything he ever knew about generating suspense\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          0,\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "CILZF7vtm15c",
        "outputId": "1e1f1047-0992-4746-c985-348bcecb005d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  label\n",
              "0       no movement , no yuks , not much of anything      0\n",
              "1  a gob of drivel so sickly sweet , even the eag...      0\n",
              "2  gangs of new york is an unapologetic mess , wh...      0\n",
              "3  we never really feel involved with the story ,...      0\n",
              "4              this is one of polanski 's best films      1"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-13f4f1ff-5014-4f12-997c-2442f581f835\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>no movement , no yuks , not much of anything</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>a gob of drivel so sickly sweet , even the eag...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>gangs of new york is an unapologetic mess , wh...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>we never really feel involved with the story ,...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>this is one of polanski 's best films</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-13f4f1ff-5014-4f12-997c-2442f581f835')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-13f4f1ff-5014-4f12-997c-2442f581f835 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-13f4f1ff-5014-4f12-997c-2442f581f835');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c131c429-305c-4d76-aa71-fb97072e46d2\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c131c429-305c-4d76-aa71-fb97072e46d2')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c131c429-305c-4d76-aa71-fb97072e46d2 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "test_df",
              "summary": "{\n  \"name\": \"test_df\",\n  \"rows\": 1821,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 1821,\n        \"samples\": [\n          \"there is no entry portal in the rules of attraction , and i spent most of the movie feeling depressed by the shallow , selfish , greedy characters\",\n          \"a charming yet poignant tale of the irrevocable ties that bind\",\n          \"it 's all pretty cynical and condescending , too\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 1,\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use 20% of the training dataset as the validation set.\n"
      ],
      "metadata": {
        "id": "Ll5iEO0Li9cV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split into train and validation sets\n",
        "train_texts, val_texts, train_labels, val_labels = train_test_split(\n",
        "    train_df[\"sentence\"], train_df[\"label\"], test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "AUGqj1gzT5nN"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct a Multi-Layer Perceptron (MLP) model."
      ],
      "metadata": {
        "id": "sp-sHJOzjCiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Initialize the vectorizer\n",
        "vectorizer = TfidfVectorizer(max_features=10000)  # You can tune max_features\n",
        "\n",
        "# Fit on training data and transform train/val\n",
        "X_train = vectorizer.fit_transform(train_texts).toarray()\n",
        "X_val = vectorizer.transform(val_texts).toarray()\n"
      ],
      "metadata": {
        "id": "B8Z6NnrNWQs9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HhVwprUIhNH8",
        "outputId": "5082eeab-3c61-42df-caeb-55c80a9a88a4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5536, 10000)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "y_train = torch.tensor(train_labels.values, dtype=torch.long)\n",
        "y_val = torch.tensor(val_labels.values, dtype=torch.long)\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_val = torch.tensor(X_val, dtype=torch.float32)\n"
      ],
      "metadata": {
        "id": "DoTxlkLOWSsh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQ4FMOi0hprf",
        "outputId": "2eb90ef4-f365-431a-b9c9-05bcbb405508"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5536, 10000])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The parameter should be with:<br>\n",
        "  hidden_sizes=[512, 256, 128, 64]<br>\n",
        "Output should have two labels.\n"
      ],
      "metadata": {
        "id": "Xtl8qiyZjNiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class MLPClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_sizes, output_dim=2):\n",
        "        super(MLPClassifier, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        in_dim = input_dim\n",
        "        for h in hidden_sizes:\n",
        "            layers.append(nn.Linear(in_dim, h))\n",
        "            layers.append(nn.ReLU())\n",
        "            in_dim = h\n",
        "        layers.append(nn.Linear(in_dim, output_dim))  # Output layer\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n"
      ],
      "metadata": {
        "id": "9JagMUG-WfhD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[1]\n",
        "hidden_sizes = [512, 256, 128, 64]\n",
        "\n",
        "model = MLPClassifier(input_dim=input_dim, hidden_sizes=hidden_sizes)\n"
      ],
      "metadata": {
        "id": "YR5cXR2mWmJR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.state_dict)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rnnbKwLNjj16",
        "outputId": "232ba5d0-2e7c-471e-9fc2-192643d0d5fe"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<bound method Module.state_dict of MLPClassifier(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=10000, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (7): ReLU()\n",
            "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
            "  )\n",
            ")>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Count the number of trainable parameters in the model using the automated function"
      ],
      "metadata": {
        "id": "AnBsR0-PlA5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(\"Total trainable parameters:\", count_parameters(model))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eawKXDEDWriL",
        "outputId": "3d9e131c-e9df-4a81-e4b3-f365ace2c344"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total trainable parameters: 5293122\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train the model with 10 epochs and create the best-performing model (checkpoint.pt)"
      ],
      "metadata": {
        "id": "eAIdK_b0lGVc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n"
      ],
      "metadata": {
        "id": "k--bqVmIXQZn"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 10\n",
        "train_losses = []\n",
        "val_accuracies = []\n",
        "best_val_acc = 0.0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    outputs = model(X_train)\n",
        "    loss = criterion(outputs, y_train)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    train_losses.append(loss.item())\n",
        "\n",
        "    # Validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_preds = model(X_val)\n",
        "        val_preds_labels = torch.argmax(val_preds, dim=1)\n",
        "        val_acc = accuracy_score(y_val.cpu(), val_preds_labels.cpu())\n",
        "        val_accuracies.append(val_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if val_acc > best_val_acc:\n",
        "            best_val_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"checkpoint.pt\")\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {loss.item():.4f} - Val Acc: {val_acc:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oPv7RdtQXVNe",
        "outputId": "55405dfd-c53c-421c-d451-33509c16ec65"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10 - Loss: 0.6929 - Val Acc: 0.5152\n",
            "Epoch 2/10 - Loss: 0.6922 - Val Acc: 0.5152\n",
            "Epoch 3/10 - Loss: 0.6913 - Val Acc: 0.5152\n",
            "Epoch 4/10 - Loss: 0.6899 - Val Acc: 0.5152\n",
            "Epoch 5/10 - Loss: 0.6879 - Val Acc: 0.5152\n",
            "Epoch 6/10 - Loss: 0.6845 - Val Acc: 0.5152\n",
            "Epoch 7/10 - Loss: 0.6798 - Val Acc: 0.5152\n",
            "Epoch 8/10 - Loss: 0.6736 - Val Acc: 0.5152\n",
            "Epoch 9/10 - Loss: 0.6658 - Val Acc: 0.5152\n",
            "Epoch 10/10 - Loss: 0.6561 - Val Acc: 0.5152\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Plot the validation accuracy + loss (epochs vs accuracy-loss)."
      ],
      "metadata": {
        "id": "nolfF1htpq6a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(range(1, epochs + 1), train_losses, marker='o')\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, epochs + 1), val_accuracies, marker='x', color='orange')\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "Qfg2GPotXfFg",
        "outputId": "687c78a4-0631-4124-feae-66b71e2e89be"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(\"checkpoint.pt\"))\n",
        "model.eval()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GU4QZJFbXxoU",
        "outputId": "2eea5f78-01c5-4d2b-9376-19b25af19813"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(\n",
              "  (model): Sequential(\n",
              "    (0): Linear(in_features=10000, out_features=512, bias=True)\n",
              "    (1): ReLU()\n",
              "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
              "    (3): ReLU()\n",
              "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
              "    (5): ReLU()\n",
              "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
              "    (7): ReLU()\n",
              "    (8): Linear(in_features=64, out_features=2, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nzFmRbW2wXKq",
        "outputId": "d7d29e34-0be2-48bc-f30d-886e404a7fb3"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('model.0.weight',\n",
              "              tensor([[-0.0033,  0.0092, -0.0031,  ..., -0.0092, -0.0089,  0.0054],\n",
              "                      [-0.0049,  0.0019,  0.0071,  ..., -0.0097, -0.0079, -0.0015],\n",
              "                      [-0.0057,  0.0039, -0.0012,  ...,  0.0043, -0.0050, -0.0081],\n",
              "                      ...,\n",
              "                      [ 0.0067,  0.0021,  0.0080,  ...,  0.0022,  0.0064,  0.0103],\n",
              "                      [-0.0014, -0.0013, -0.0027,  ...,  0.0001,  0.0088,  0.0092],\n",
              "                      [-0.0100,  0.0066, -0.0061,  ...,  0.0035, -0.0028, -0.0003]])),\n",
              "             ('model.0.bias',\n",
              "              tensor([-7.6554e-03, -4.7924e-03,  5.4157e-03, -3.4969e-03, -5.9657e-03,\n",
              "                       6.1831e-03,  4.4844e-03,  4.3857e-03,  5.6201e-03, -2.5374e-03,\n",
              "                       4.8076e-03, -3.8189e-03,  4.6759e-03, -2.1343e-03, -1.2640e-04,\n",
              "                       8.2336e-03,  4.2781e-04,  3.9007e-03, -7.9150e-03, -4.6813e-03,\n",
              "                      -6.4016e-03, -2.1817e-04,  3.4058e-03, -7.6454e-03, -1.3288e-03,\n",
              "                       4.9625e-03, -3.9016e-03, -3.9560e-03,  3.8683e-03, -5.8990e-03,\n",
              "                       7.8875e-03, -4.0040e-03, -6.8808e-03, -6.1360e-03,  9.9611e-03,\n",
              "                      -2.2421e-03,  3.3507e-03, -4.3928e-03,  3.7023e-03,  6.1306e-03,\n",
              "                      -8.3086e-04, -3.5726e-03,  7.1463e-03,  7.0295e-03,  5.5936e-03,\n",
              "                      -1.9677e-03,  6.8305e-03,  7.7254e-03,  1.4139e-03,  9.7745e-03,\n",
              "                      -3.1674e-03, -1.0314e-02,  1.0633e-02,  8.7574e-03, -6.2166e-03,\n",
              "                       1.6507e-03,  1.0153e-02, -6.5799e-03, -4.3833e-05,  6.0919e-03,\n",
              "                       3.7989e-03,  5.5907e-03, -5.6625e-03,  9.0059e-03,  1.0588e-02,\n",
              "                      -1.0834e-02, -9.0031e-03,  7.9903e-03, -1.8297e-03,  9.5188e-03,\n",
              "                      -7.7680e-03,  5.3291e-04,  1.4315e-03,  7.3955e-03,  7.0978e-03,\n",
              "                      -2.7736e-03,  5.6829e-03, -2.3973e-03, -7.4940e-03,  8.3858e-03,\n",
              "                       6.4938e-03,  7.9966e-03,  2.7314e-03,  5.0863e-04,  7.8395e-03,\n",
              "                       6.2566e-03, -9.7008e-03, -5.7852e-03, -3.7211e-03, -7.7264e-03,\n",
              "                       1.0271e-02,  6.0669e-03,  2.3296e-04, -5.0106e-03,  5.7428e-03,\n",
              "                      -8.4724e-03, -1.6183e-03, -2.9391e-03,  1.8545e-03,  5.7789e-03,\n",
              "                      -1.7404e-03, -8.8310e-03, -7.6744e-03, -7.4870e-03,  7.5005e-03,\n",
              "                       5.6683e-04,  1.5218e-03, -2.4502e-03,  4.2712e-04, -1.0457e-02,\n",
              "                      -8.1870e-03,  1.0639e-02, -2.2733e-03,  3.4901e-03, -5.0670e-03,\n",
              "                       2.0986e-04, -2.7513e-04, -7.9193e-03,  5.9520e-03, -2.1375e-03,\n",
              "                       2.1920e-03,  9.3984e-04, -8.0488e-03, -1.3604e-03, -4.4572e-03,\n",
              "                      -3.1084e-03, -3.2583e-03,  6.9161e-03, -2.3179e-03,  2.6515e-03,\n",
              "                       3.3557e-03,  3.5153e-03, -3.9739e-03,  7.1481e-03, -8.1612e-04,\n",
              "                      -7.9456e-04, -1.2435e-03, -9.8542e-03,  4.1251e-03,  7.3103e-03,\n",
              "                      -2.5752e-03, -7.3650e-03,  2.3324e-03,  7.5567e-03, -1.0383e-02,\n",
              "                       4.8930e-03,  2.0703e-03, -8.7478e-03,  2.7839e-03,  1.9850e-03,\n",
              "                       2.8667e-03, -4.0996e-03, -2.9680e-03, -5.0848e-03,  7.0195e-03,\n",
              "                      -7.7236e-03,  2.9575e-03,  2.8474e-03,  3.7349e-03,  2.1917e-03,\n",
              "                      -7.2450e-04, -4.7459e-03,  7.0379e-03,  1.0380e-02, -9.7960e-04,\n",
              "                       3.6113e-04, -7.5531e-04, -8.9075e-03, -8.8732e-03, -1.0660e-02,\n",
              "                      -6.7338e-03, -6.7288e-03, -1.6707e-03,  1.1140e-03,  9.6544e-03,\n",
              "                      -5.8814e-03,  7.8334e-03,  4.0183e-03,  3.2685e-03,  9.6875e-03,\n",
              "                      -7.5928e-03,  5.5267e-03, -3.5958e-03,  6.8664e-04, -2.9152e-03,\n",
              "                       5.0996e-03, -8.1604e-03,  1.0269e-02,  6.2748e-03,  6.4231e-03,\n",
              "                      -3.7877e-03,  6.6913e-03, -6.6379e-03,  6.9938e-03, -3.1406e-03,\n",
              "                      -9.0387e-03, -4.9467e-03,  7.5917e-03,  6.7931e-03, -3.5962e-03,\n",
              "                      -6.0412e-03,  2.4044e-03, -2.4121e-03, -5.4613e-03, -4.8593e-03,\n",
              "                      -4.7988e-04, -8.8472e-04,  1.6038e-03,  4.5635e-03, -5.8035e-03,\n",
              "                      -5.6371e-03,  7.7405e-03, -2.0480e-03,  6.3510e-03, -5.3641e-03,\n",
              "                       1.0568e-02,  1.4339e-03, -1.0506e-03, -5.3077e-03,  7.7702e-03,\n",
              "                       5.8967e-03,  2.3390e-04, -3.1967e-03, -6.2807e-03, -1.2707e-03,\n",
              "                      -4.1378e-03,  3.4304e-03,  2.7418e-03, -6.9607e-03, -5.3034e-03,\n",
              "                      -1.0940e-02,  1.2748e-03,  6.6238e-03, -5.7073e-03, -5.3212e-03,\n",
              "                      -6.2279e-03,  9.2957e-03, -5.9801e-03,  1.0015e-02, -5.9047e-03,\n",
              "                      -8.7623e-03, -3.9302e-03,  9.0407e-03,  8.7167e-03, -8.7072e-04,\n",
              "                       1.0314e-02, -7.1160e-04, -5.2259e-03, -4.2422e-03, -3.4969e-03,\n",
              "                       8.3894e-03,  6.3889e-03,  1.0159e-02,  9.9233e-03,  7.5948e-03,\n",
              "                       2.2056e-03,  3.4271e-03, -2.3847e-04,  3.2949e-04,  7.2473e-03,\n",
              "                       2.2510e-03, -8.9347e-03,  1.0213e-02,  8.1530e-03, -4.0832e-03,\n",
              "                       6.4352e-04,  3.4154e-03,  3.9475e-03, -6.4537e-04, -7.8672e-04,\n",
              "                      -4.8040e-03, -8.9294e-03, -9.6105e-03,  4.3042e-03,  2.8693e-03,\n",
              "                      -7.3920e-03, -7.5635e-03, -8.3695e-03,  6.7510e-03,  1.0807e-02,\n",
              "                      -7.1183e-03, -3.8127e-04, -3.1467e-03,  7.9895e-03,  8.4601e-03,\n",
              "                      -8.7870e-03, -8.1250e-03,  2.0239e-04,  7.7867e-03, -1.3468e-04,\n",
              "                       3.6955e-03, -4.3773e-03, -8.6291e-03,  2.4751e-03,  2.4460e-03,\n",
              "                       1.9345e-03, -3.4684e-03, -7.9477e-03,  6.2079e-03,  6.6678e-03,\n",
              "                       3.5781e-03,  8.7123e-03,  1.7533e-03,  3.4010e-03,  5.9777e-03,\n",
              "                      -7.8856e-03, -5.1612e-03,  6.7411e-03,  1.0994e-02,  4.7787e-03,\n",
              "                       3.2921e-03,  9.1656e-03,  1.3560e-03,  7.0880e-03,  6.4780e-03,\n",
              "                      -3.5377e-03,  8.1345e-03, -4.5393e-03, -3.4726e-03, -5.6400e-03,\n",
              "                      -5.4519e-03,  8.2557e-03,  1.5337e-03,  9.7327e-03,  6.1963e-03,\n",
              "                       8.8256e-03,  9.3832e-03, -8.4428e-03, -3.7721e-03, -1.8288e-03,\n",
              "                       1.0403e-02, -1.0355e-02,  5.2217e-03, -2.0061e-03, -6.2602e-03,\n",
              "                      -9.5476e-05,  6.1724e-03, -7.8473e-03,  5.4229e-03, -4.3577e-03,\n",
              "                       5.5142e-03, -3.2796e-03,  1.3079e-03, -7.6600e-03, -7.7008e-03,\n",
              "                      -9.0519e-03,  3.6916e-03,  8.2886e-04, -4.8266e-04, -7.0475e-03,\n",
              "                      -9.8792e-03, -6.7929e-03, -2.4956e-03, -3.3827e-03,  6.3309e-03,\n",
              "                       6.1973e-03,  6.0983e-03, -7.3339e-03,  8.3331e-05,  1.8361e-03,\n",
              "                       5.4275e-03,  1.0640e-02, -4.3196e-03,  6.6189e-03,  4.7652e-03,\n",
              "                       9.9756e-04, -5.5147e-03, -3.0770e-03,  5.6816e-03,  8.5766e-03,\n",
              "                      -6.6772e-03,  1.2639e-03, -8.2120e-03, -8.7945e-03, -1.8505e-03,\n",
              "                       9.4866e-03,  3.0225e-03,  9.1949e-03,  3.2566e-03, -4.5941e-03,\n",
              "                      -7.7843e-03,  6.0286e-03, -6.9483e-03,  2.7482e-03, -7.3948e-03,\n",
              "                       6.2860e-03, -4.8992e-03,  4.2554e-03,  3.3871e-03, -9.4632e-03,\n",
              "                      -6.3830e-03, -3.3265e-03, -4.3975e-03,  8.9130e-03, -5.6275e-04,\n",
              "                       3.7128e-03, -8.3674e-03, -1.0310e-02,  3.3743e-03, -8.8399e-03,\n",
              "                       6.5925e-03,  7.3868e-03,  6.0704e-03, -9.1660e-03, -3.3096e-03,\n",
              "                       6.3438e-03,  8.9259e-03, -5.4175e-03,  1.7854e-03,  6.7975e-03,\n",
              "                       6.1265e-03, -6.5741e-03,  5.8448e-03,  3.0257e-03, -6.7106e-04,\n",
              "                       3.7996e-03, -1.0774e-03, -1.8748e-03, -3.7294e-03, -2.0235e-03,\n",
              "                      -2.5069e-03, -6.0832e-03,  6.2051e-03,  4.4905e-03, -2.2730e-03,\n",
              "                       3.9221e-03,  2.3405e-03, -9.4231e-04, -9.7648e-04, -5.3345e-03,\n",
              "                      -5.9692e-04, -3.3741e-03,  2.1329e-03, -9.2209e-03, -9.0203e-04,\n",
              "                       6.6513e-03,  4.6466e-03,  4.5963e-03, -1.6836e-03,  5.7518e-03,\n",
              "                       5.4453e-03,  7.5723e-04, -1.5492e-03, -3.2662e-05, -7.2432e-03,\n",
              "                      -1.7222e-03,  9.8560e-03,  6.1800e-03,  2.2950e-03,  1.2232e-03,\n",
              "                      -3.4648e-03,  9.9650e-03, -4.8749e-03, -2.2308e-03, -7.0467e-03,\n",
              "                      -5.8004e-03,  3.6390e-03, -7.7843e-03, -1.7641e-03,  6.3329e-03,\n",
              "                       8.9389e-03,  6.7366e-03, -5.8310e-03, -8.8420e-04,  8.5654e-04,\n",
              "                      -3.8893e-03, -2.6870e-03,  5.9040e-03,  6.3937e-03,  8.8051e-03,\n",
              "                      -6.0526e-03, -1.3818e-03,  4.9880e-04,  2.4970e-03,  9.7006e-04,\n",
              "                       3.0888e-03, -8.4180e-03, -6.3999e-03, -5.4668e-03,  7.5140e-03,\n",
              "                       1.1632e-03,  1.0863e-02, -4.1625e-03,  8.2678e-04,  7.8006e-03,\n",
              "                      -3.0362e-04, -4.3857e-04,  1.8640e-03,  6.4618e-03,  5.3044e-03,\n",
              "                      -1.4936e-03, -2.7580e-04,  8.5333e-03, -1.9467e-03,  8.7842e-03,\n",
              "                      -6.6787e-03, -5.3916e-03, -6.3432e-03, -1.0405e-02, -2.0232e-03,\n",
              "                      -6.9507e-03, -3.2018e-03, -1.2804e-03, -1.9848e-04,  7.1640e-03,\n",
              "                      -5.7252e-03, -5.0713e-03, -4.5148e-03,  5.2529e-03,  1.0634e-02,\n",
              "                      -7.0448e-03,  3.8926e-03])),\n",
              "             ('model.2.weight',\n",
              "              tensor([[-0.0223,  0.0201,  0.0326,  ...,  0.0032,  0.0315, -0.0224],\n",
              "                      [-0.0111,  0.0067,  0.0038,  ..., -0.0255, -0.0333, -0.0439],\n",
              "                      [-0.0028,  0.0343, -0.0208,  ...,  0.0271,  0.0096,  0.0262],\n",
              "                      ...,\n",
              "                      [ 0.0320,  0.0066,  0.0143,  ..., -0.0206,  0.0014, -0.0406],\n",
              "                      [ 0.0026, -0.0271,  0.0064,  ..., -0.0210,  0.0363, -0.0201],\n",
              "                      [-0.0013, -0.0205, -0.0225,  ...,  0.0091, -0.0380, -0.0153]])),\n",
              "             ('model.2.bias',\n",
              "              tensor([-0.0439,  0.0052, -0.0086, -0.0230,  0.0143,  0.0141, -0.0116,  0.0374,\n",
              "                       0.0152, -0.0161, -0.0063, -0.0039,  0.0326,  0.0418, -0.0020, -0.0197,\n",
              "                      -0.0175, -0.0290,  0.0196, -0.0172, -0.0033, -0.0244, -0.0398,  0.0147,\n",
              "                       0.0127, -0.0378,  0.0257,  0.0127, -0.0103,  0.0186, -0.0085,  0.0160,\n",
              "                      -0.0377, -0.0022, -0.0278, -0.0381, -0.0083, -0.0213,  0.0089, -0.0034,\n",
              "                       0.0164,  0.0119,  0.0347, -0.0354, -0.0341, -0.0310, -0.0050,  0.0196,\n",
              "                       0.0230,  0.0206, -0.0416, -0.0246,  0.0295,  0.0053,  0.0071,  0.0354,\n",
              "                      -0.0094,  0.0090, -0.0308, -0.0381,  0.0305,  0.0159, -0.0261, -0.0165,\n",
              "                      -0.0158,  0.0234, -0.0074, -0.0202,  0.0359,  0.0240, -0.0134,  0.0342,\n",
              "                       0.0283, -0.0099, -0.0115,  0.0265,  0.0040,  0.0375,  0.0340,  0.0219,\n",
              "                      -0.0369, -0.0219, -0.0098,  0.0330,  0.0358, -0.0149,  0.0408,  0.0138,\n",
              "                       0.0275, -0.0438,  0.0427,  0.0285,  0.0232, -0.0219, -0.0409, -0.0237,\n",
              "                       0.0313,  0.0373,  0.0181,  0.0116, -0.0270, -0.0246, -0.0350,  0.0180,\n",
              "                      -0.0157, -0.0210, -0.0306,  0.0393,  0.0038,  0.0408, -0.0329,  0.0387,\n",
              "                      -0.0160, -0.0411, -0.0319, -0.0359,  0.0255, -0.0016,  0.0203,  0.0170,\n",
              "                      -0.0080, -0.0348,  0.0182,  0.0244, -0.0214,  0.0309, -0.0042, -0.0268,\n",
              "                      -0.0436, -0.0329, -0.0055, -0.0377, -0.0311, -0.0231, -0.0210, -0.0162,\n",
              "                      -0.0172,  0.0355, -0.0390, -0.0387,  0.0202, -0.0406,  0.0442, -0.0339,\n",
              "                       0.0034, -0.0328,  0.0370, -0.0214, -0.0402,  0.0371,  0.0194,  0.0368,\n",
              "                      -0.0390,  0.0414, -0.0087,  0.0106,  0.0219, -0.0046, -0.0319,  0.0290,\n",
              "                      -0.0105, -0.0333, -0.0378,  0.0090,  0.0271, -0.0362,  0.0157,  0.0421,\n",
              "                      -0.0313,  0.0098, -0.0319, -0.0388,  0.0019, -0.0027, -0.0109, -0.0142,\n",
              "                      -0.0209, -0.0071,  0.0307,  0.0324,  0.0104, -0.0092,  0.0107, -0.0240,\n",
              "                       0.0188,  0.0348, -0.0272,  0.0239, -0.0267,  0.0305, -0.0222, -0.0278,\n",
              "                      -0.0246, -0.0405,  0.0156, -0.0103, -0.0320, -0.0358, -0.0094,  0.0306,\n",
              "                      -0.0328,  0.0253, -0.0187, -0.0084, -0.0048,  0.0092,  0.0296,  0.0122,\n",
              "                       0.0390, -0.0043, -0.0058, -0.0419, -0.0111,  0.0309, -0.0043, -0.0032,\n",
              "                       0.0240,  0.0226, -0.0332, -0.0208, -0.0082, -0.0381,  0.0339,  0.0125,\n",
              "                      -0.0247,  0.0366,  0.0358, -0.0030,  0.0398,  0.0043, -0.0259, -0.0412,\n",
              "                      -0.0055,  0.0184,  0.0098, -0.0061, -0.0304, -0.0209, -0.0336, -0.0208,\n",
              "                       0.0431, -0.0123, -0.0346, -0.0130,  0.0086,  0.0312, -0.0350, -0.0353,\n",
              "                       0.0112,  0.0427, -0.0303,  0.0200,  0.0201,  0.0355,  0.0206, -0.0366])),\n",
              "             ('model.4.weight',\n",
              "              tensor([[-0.0526,  0.0314, -0.0577,  ..., -0.0249,  0.0487, -0.0196],\n",
              "                      [-0.0280,  0.0009,  0.0427,  ..., -0.0193,  0.0629,  0.0077],\n",
              "                      [ 0.0137, -0.0197,  0.0406,  ...,  0.0229,  0.0557,  0.0202],\n",
              "                      ...,\n",
              "                      [-0.0061, -0.0441,  0.0616,  ..., -0.0018,  0.0025, -0.0403],\n",
              "                      [-0.0208, -0.0319, -0.0354,  ...,  0.0145, -0.0177, -0.0591],\n",
              "                      [-0.0124, -0.0532, -0.0340,  ..., -0.0123,  0.0069,  0.0314]])),\n",
              "             ('model.4.bias',\n",
              "              tensor([-0.0109,  0.0403, -0.0146,  0.0250,  0.0158, -0.0059,  0.0012,  0.0172,\n",
              "                      -0.0430, -0.0376, -0.0060,  0.0471,  0.0558, -0.0373,  0.0233, -0.0497,\n",
              "                       0.0476,  0.0512,  0.0478,  0.0126, -0.0156, -0.0182, -0.0267, -0.0165,\n",
              "                       0.0050,  0.0528,  0.0499,  0.0176,  0.0403,  0.0349, -0.0136, -0.0537,\n",
              "                       0.0504, -0.0038,  0.0117,  0.0245, -0.0475, -0.0277, -0.0058, -0.0219,\n",
              "                      -0.0396,  0.0509, -0.0491, -0.0448,  0.0096, -0.0172,  0.0561,  0.0328,\n",
              "                      -0.0573, -0.0192,  0.0067, -0.0482, -0.0471, -0.0499, -0.0226, -0.0082,\n",
              "                       0.0585,  0.0089,  0.0526,  0.0413,  0.0383, -0.0107,  0.0237, -0.0227,\n",
              "                      -0.0234,  0.0201, -0.0088, -0.0573,  0.0029,  0.0237, -0.0138,  0.0288,\n",
              "                       0.0336,  0.0178, -0.0345,  0.0171,  0.0437,  0.0537, -0.0531,  0.0343,\n",
              "                       0.0468,  0.0189, -0.0455,  0.0478,  0.0255, -0.0088,  0.0040, -0.0325,\n",
              "                       0.0372,  0.0465,  0.0009,  0.0529, -0.0289, -0.0183, -0.0395,  0.0216,\n",
              "                       0.0541,  0.0035,  0.0611,  0.0393, -0.0036,  0.0518,  0.0613,  0.0019,\n",
              "                      -0.0298, -0.0566, -0.0154, -0.0169,  0.0183,  0.0118,  0.0488,  0.0043,\n",
              "                      -0.0579, -0.0248,  0.0114,  0.0154,  0.0430,  0.0132, -0.0367, -0.0393,\n",
              "                      -0.0205,  0.0613, -0.0351, -0.0310, -0.0305, -0.0454,  0.0424,  0.0016])),\n",
              "             ('model.6.weight',\n",
              "              tensor([[ 0.0095, -0.0058, -0.0875,  ...,  0.0073,  0.0746,  0.0162],\n",
              "                      [-0.0773, -0.0870, -0.0435,  ..., -0.0373,  0.0312,  0.0039],\n",
              "                      [ 0.0383, -0.0494, -0.0582,  ...,  0.0650,  0.0605, -0.0449],\n",
              "                      ...,\n",
              "                      [ 0.0215,  0.0115, -0.0877,  ..., -0.0709, -0.0717,  0.0351],\n",
              "                      [-0.0153,  0.0020,  0.0548,  ...,  0.0190,  0.0391, -0.0033],\n",
              "                      [-0.0683, -0.0664,  0.0436,  ...,  0.0221, -0.0775,  0.0451]])),\n",
              "             ('model.6.bias',\n",
              "              tensor([-0.0482, -0.0485,  0.0675, -0.0445,  0.0564,  0.0664,  0.0333,  0.0358,\n",
              "                      -0.0288,  0.0576,  0.0534, -0.0360, -0.0061,  0.0117, -0.0729,  0.0278,\n",
              "                      -0.0781, -0.0365,  0.0113,  0.0220,  0.0188, -0.0362,  0.0660,  0.0245,\n",
              "                       0.0279,  0.0296,  0.0208,  0.0640,  0.0105, -0.0225,  0.0575, -0.0566,\n",
              "                      -0.0049, -0.0277,  0.0636, -0.0016,  0.0002, -0.0691,  0.0653,  0.0069,\n",
              "                       0.0185, -0.0136,  0.0556,  0.0661, -0.0306,  0.0140, -0.0367,  0.0547,\n",
              "                      -0.0048,  0.0543,  0.0572, -0.0512,  0.0644, -0.0068, -0.0605,  0.0509,\n",
              "                       0.0400, -0.0096, -0.0314,  0.0286,  0.0364, -0.0568, -0.0547,  0.0236])),\n",
              "             ('model.8.weight',\n",
              "              tensor([[-1.1375e-01,  1.1035e-01, -3.4615e-02, -6.6058e-02, -1.0690e-01,\n",
              "                       -2.3935e-02, -2.5241e-02, -1.0330e-01,  7.6849e-02, -3.8399e-02,\n",
              "                       -2.0886e-02, -5.4110e-02, -6.0885e-02, -6.1246e-02, -1.8391e-02,\n",
              "                        1.0949e-01,  8.4079e-02, -7.1850e-02,  1.1656e-01,  1.1965e-01,\n",
              "                        3.9550e-02,  9.5327e-02,  2.2964e-02,  6.4664e-04, -4.7852e-02,\n",
              "                       -5.1961e-03, -4.7309e-02,  1.0296e-01, -5.1350e-02,  8.2736e-02,\n",
              "                        4.9918e-02, -2.6597e-03, -1.3835e-02,  8.9034e-02, -1.4029e-03,\n",
              "                        1.2056e-02,  8.4545e-02, -1.0534e-01, -1.1020e-01,  9.0085e-02,\n",
              "                       -5.3881e-02,  6.7496e-02, -6.1141e-03, -3.0590e-02,  1.1339e-02,\n",
              "                        9.2045e-02,  5.7999e-02,  5.9192e-02,  1.1182e-01, -9.0997e-02,\n",
              "                        1.7836e-02, -1.5166e-02, -3.8379e-02, -1.0712e-01, -1.5407e-02,\n",
              "                       -4.4958e-02,  8.4614e-02, -9.4442e-02,  1.1068e-01,  6.4682e-02,\n",
              "                       -2.7906e-02, -7.0747e-02, -4.2093e-02, -4.0915e-02],\n",
              "                      [ 1.1668e-04,  1.1726e-01, -7.4390e-02, -9.0714e-02,  4.4167e-02,\n",
              "                        6.3389e-02,  3.6321e-02, -4.3871e-02,  5.3064e-02,  7.0861e-02,\n",
              "                       -6.2652e-02,  5.4337e-02,  1.1804e-01,  3.7951e-03, -3.3385e-02,\n",
              "                        5.0512e-02,  1.4124e-02,  6.3418e-02,  1.2500e-01,  1.2975e-02,\n",
              "                        8.7003e-02, -3.3008e-02, -4.8196e-02,  7.3142e-02,  1.1715e-01,\n",
              "                       -2.8734e-03, -5.6483e-02,  1.2560e-01,  1.0152e-01, -6.7530e-02,\n",
              "                        1.0442e-01,  1.7612e-02,  1.2418e-01,  9.6447e-02, -1.1259e-01,\n",
              "                       -7.6689e-02, -4.6774e-02, -3.8225e-02,  5.3317e-02, -9.0824e-02,\n",
              "                       -6.0371e-02,  6.5965e-02, -1.1623e-01,  2.7731e-02, -2.2358e-02,\n",
              "                        5.3064e-02,  2.0709e-02,  3.0872e-02,  8.9177e-02,  5.3164e-02,\n",
              "                       -9.2296e-02, -5.2422e-02, -1.2113e-01, -2.3268e-02,  9.5861e-02,\n",
              "                       -4.7631e-02, -1.1136e-01, -5.1045e-03,  8.3597e-02, -5.7867e-02,\n",
              "                        4.0600e-02,  9.9563e-02, -3.1066e-02,  1.1057e-01]])),\n",
              "             ('model.8.bias', tensor([-0.0984, -0.0943]))])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_texts = test_df[\"sentence\"]\n",
        "X_test = vectorizer.transform(test_texts).toarray()\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "test_labels = test_df[\"label\"]\n",
        "y_test = torch.tensor(test_labels.values, dtype=torch.long)"
      ],
      "metadata": {
        "id": "zecdaJfzom2l"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def get_avg_inference_time(model, X_test, num_runs=10):\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            start = time.time()\n",
        "            _ = model(X_test)\n",
        "            end = time.time()\n",
        "            times.append((end - start) * 1000)\n",
        "    return sum(times) / len(times)  # in ms\n"
      ],
      "metadata": {
        "id": "bPCuQKx0PMmz"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(X_test)\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "orig_acc = accuracy_score(y_test.cpu(), preds.cpu())\n",
        "print(f\"[Original] Accuracy: {orig_acc:.4f}\")\n",
        "orig_time = get_avg_inference_time(model, X_test)\n",
        "print(f\"[Original] Inference Time: {orig_time:.2f} ms\")\n",
        "torch.save(model.state_dict(), \"original.pt\")\n",
        "orig_size = os.path.getsize(\"original.pt\") / (1024 * 1024)\n",
        "print(f\"[Original] Model Size: {orig_size:.2f} MB\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CoUWTYifnl5o",
        "outputId": "dc0e8959-f062-4489-b026-a3da6d483227"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Original] Accuracy: 0.4992\n",
            "[Original] Inference Time: 313.35 ms\n",
            "[Original] Model Size: 20.20 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dynamic Quantization with INT4"
      ],
      "metadata": {
        "id": "ojIw_d-Rqo3-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Only linear layers can be quantized dynamically\n",
        "quantized_model = torch.quantization.quantize_dynamic(\n",
        "    copy.deepcopy(model),  # make a copy of the model\n",
        "    {nn.Linear},  # only quantize Linear layers\n",
        "    dtype=torch.qint8  # use INT8 quantization\n",
        ")\n"
      ],
      "metadata": {
        "id": "Wdu85JGRX2GH"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = quantized_model(X_test)\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "quantized_accuracy = accuracy_score(y_test, preds)\n",
        "inference_time_ms = get_avg_inference_time(quantized_model, X_test)\n",
        "\n",
        "print(f\"[Dynamic Quantization] Accuracy: {quantized_accuracy:.4f}, Inference Time: {inference_time_ms:.2f} ms\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SgMOkTTiX9Na",
        "outputId": "127a2ee7-c037-44c6-f967-11353406e23c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dynamic Quantization] Accuracy: 0.4992, Inference Time: 214.24 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(quantized_model.state_dict(), \"dynamic_quantized.pt\")\n",
        "\n",
        "import os\n",
        "size_mb = os.path.getsize(\"dynamic_quantized.pt\") / (1024 * 1024)\n",
        "print(f\"[Dynamic Quantization] Model Size: {size_mb:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0FExmQLYIPn",
        "outputId": "63394ab9-f809-4151-af25-b42f1f0c4456"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dynamic Quantization] Model Size: 5.06 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "quantized_model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8M2osJOjwHiJ",
        "outputId": "374c19d2-f15a-4606-bb1f-b37943ed09e5"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('model.0.scale', tensor(1.)),\n",
              "             ('model.0.zero_point', tensor(0)),\n",
              "             ('model.0._packed_params.dtype', torch.qint8),\n",
              "             ('model.0._packed_params._packed_params',\n",
              "              (tensor([[-3.2772e-03,  9.2280e-03, -3.1048e-03,  ..., -9.1418e-03,\n",
              "                        -8.8830e-03,  5.4333e-03],\n",
              "                       [-4.8296e-03,  1.8973e-03,  7.0719e-03,  ..., -9.6592e-03,\n",
              "                        -7.9344e-03, -1.5524e-03],\n",
              "                       [-5.6920e-03,  3.9672e-03, -1.2074e-03,  ...,  4.3122e-03,\n",
              "                        -5.0021e-03, -8.0206e-03],\n",
              "                       ...,\n",
              "                       [ 6.7270e-03,  2.0698e-03,  8.0206e-03,  ...,  2.2423e-03,\n",
              "                         6.3820e-03,  1.0349e-02],\n",
              "                       [-1.3799e-03, -1.2936e-03, -2.7598e-03,  ...,  8.6243e-05,\n",
              "                         8.7968e-03,  9.2280e-03],\n",
              "                       [-1.0004e-02,  6.6407e-03, -6.1233e-03,  ...,  3.4497e-03,\n",
              "                        -2.7598e-03, -3.4497e-04]], size=(512, 10000), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=8.624310430604964e-05,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-7.6554e-03, -4.7924e-03,  5.4157e-03, -3.4969e-03, -5.9657e-03,\n",
              "                        6.1831e-03,  4.4844e-03,  4.3857e-03,  5.6201e-03, -2.5374e-03,\n",
              "                        4.8076e-03, -3.8189e-03,  4.6759e-03, -2.1343e-03, -1.2640e-04,\n",
              "                        8.2336e-03,  4.2781e-04,  3.9007e-03, -7.9150e-03, -4.6813e-03,\n",
              "                       -6.4016e-03, -2.1817e-04,  3.4058e-03, -7.6454e-03, -1.3288e-03,\n",
              "                        4.9625e-03, -3.9016e-03, -3.9560e-03,  3.8683e-03, -5.8990e-03,\n",
              "                        7.8875e-03, -4.0040e-03, -6.8808e-03, -6.1360e-03,  9.9611e-03,\n",
              "                       -2.2421e-03,  3.3507e-03, -4.3928e-03,  3.7023e-03,  6.1306e-03,\n",
              "                       -8.3086e-04, -3.5726e-03,  7.1463e-03,  7.0295e-03,  5.5936e-03,\n",
              "                       -1.9677e-03,  6.8305e-03,  7.7254e-03,  1.4139e-03,  9.7745e-03,\n",
              "                       -3.1674e-03, -1.0314e-02,  1.0633e-02,  8.7574e-03, -6.2166e-03,\n",
              "                        1.6507e-03,  1.0153e-02, -6.5799e-03, -4.3833e-05,  6.0919e-03,\n",
              "                        3.7989e-03,  5.5907e-03, -5.6625e-03,  9.0059e-03,  1.0588e-02,\n",
              "                       -1.0834e-02, -9.0031e-03,  7.9903e-03, -1.8297e-03,  9.5188e-03,\n",
              "                       -7.7680e-03,  5.3291e-04,  1.4315e-03,  7.3955e-03,  7.0978e-03,\n",
              "                       -2.7736e-03,  5.6829e-03, -2.3973e-03, -7.4940e-03,  8.3858e-03,\n",
              "                        6.4938e-03,  7.9966e-03,  2.7314e-03,  5.0863e-04,  7.8395e-03,\n",
              "                        6.2566e-03, -9.7008e-03, -5.7852e-03, -3.7211e-03, -7.7264e-03,\n",
              "                        1.0271e-02,  6.0669e-03,  2.3296e-04, -5.0106e-03,  5.7428e-03,\n",
              "                       -8.4724e-03, -1.6183e-03, -2.9391e-03,  1.8545e-03,  5.7789e-03,\n",
              "                       -1.7404e-03, -8.8310e-03, -7.6744e-03, -7.4870e-03,  7.5005e-03,\n",
              "                        5.6683e-04,  1.5218e-03, -2.4502e-03,  4.2712e-04, -1.0457e-02,\n",
              "                       -8.1870e-03,  1.0639e-02, -2.2733e-03,  3.4901e-03, -5.0670e-03,\n",
              "                        2.0986e-04, -2.7513e-04, -7.9193e-03,  5.9520e-03, -2.1375e-03,\n",
              "                        2.1920e-03,  9.3984e-04, -8.0488e-03, -1.3604e-03, -4.4572e-03,\n",
              "                       -3.1084e-03, -3.2583e-03,  6.9161e-03, -2.3179e-03,  2.6515e-03,\n",
              "                        3.3557e-03,  3.5153e-03, -3.9739e-03,  7.1481e-03, -8.1612e-04,\n",
              "                       -7.9456e-04, -1.2435e-03, -9.8542e-03,  4.1251e-03,  7.3103e-03,\n",
              "                       -2.5752e-03, -7.3650e-03,  2.3324e-03,  7.5567e-03, -1.0383e-02,\n",
              "                        4.8930e-03,  2.0703e-03, -8.7478e-03,  2.7839e-03,  1.9850e-03,\n",
              "                        2.8667e-03, -4.0996e-03, -2.9680e-03, -5.0848e-03,  7.0195e-03,\n",
              "                       -7.7236e-03,  2.9575e-03,  2.8474e-03,  3.7349e-03,  2.1917e-03,\n",
              "                       -7.2450e-04, -4.7459e-03,  7.0379e-03,  1.0380e-02, -9.7960e-04,\n",
              "                        3.6113e-04, -7.5531e-04, -8.9075e-03, -8.8732e-03, -1.0660e-02,\n",
              "                       -6.7338e-03, -6.7288e-03, -1.6707e-03,  1.1140e-03,  9.6544e-03,\n",
              "                       -5.8814e-03,  7.8334e-03,  4.0183e-03,  3.2685e-03,  9.6875e-03,\n",
              "                       -7.5928e-03,  5.5267e-03, -3.5958e-03,  6.8664e-04, -2.9152e-03,\n",
              "                        5.0996e-03, -8.1604e-03,  1.0269e-02,  6.2748e-03,  6.4231e-03,\n",
              "                       -3.7877e-03,  6.6913e-03, -6.6379e-03,  6.9938e-03, -3.1406e-03,\n",
              "                       -9.0387e-03, -4.9467e-03,  7.5917e-03,  6.7931e-03, -3.5962e-03,\n",
              "                       -6.0412e-03,  2.4044e-03, -2.4121e-03, -5.4613e-03, -4.8593e-03,\n",
              "                       -4.7988e-04, -8.8472e-04,  1.6038e-03,  4.5635e-03, -5.8035e-03,\n",
              "                       -5.6371e-03,  7.7405e-03, -2.0480e-03,  6.3510e-03, -5.3641e-03,\n",
              "                        1.0568e-02,  1.4339e-03, -1.0506e-03, -5.3077e-03,  7.7702e-03,\n",
              "                        5.8967e-03,  2.3390e-04, -3.1967e-03, -6.2807e-03, -1.2707e-03,\n",
              "                       -4.1378e-03,  3.4304e-03,  2.7418e-03, -6.9607e-03, -5.3034e-03,\n",
              "                       -1.0940e-02,  1.2748e-03,  6.6238e-03, -5.7073e-03, -5.3212e-03,\n",
              "                       -6.2279e-03,  9.2957e-03, -5.9801e-03,  1.0015e-02, -5.9047e-03,\n",
              "                       -8.7623e-03, -3.9302e-03,  9.0407e-03,  8.7167e-03, -8.7072e-04,\n",
              "                        1.0314e-02, -7.1160e-04, -5.2259e-03, -4.2422e-03, -3.4969e-03,\n",
              "                        8.3894e-03,  6.3889e-03,  1.0159e-02,  9.9233e-03,  7.5948e-03,\n",
              "                        2.2056e-03,  3.4271e-03, -2.3847e-04,  3.2949e-04,  7.2473e-03,\n",
              "                        2.2510e-03, -8.9347e-03,  1.0213e-02,  8.1530e-03, -4.0832e-03,\n",
              "                        6.4352e-04,  3.4154e-03,  3.9475e-03, -6.4537e-04, -7.8672e-04,\n",
              "                       -4.8040e-03, -8.9294e-03, -9.6105e-03,  4.3042e-03,  2.8693e-03,\n",
              "                       -7.3920e-03, -7.5635e-03, -8.3695e-03,  6.7510e-03,  1.0807e-02,\n",
              "                       -7.1183e-03, -3.8127e-04, -3.1467e-03,  7.9895e-03,  8.4601e-03,\n",
              "                       -8.7870e-03, -8.1250e-03,  2.0239e-04,  7.7867e-03, -1.3468e-04,\n",
              "                        3.6955e-03, -4.3773e-03, -8.6291e-03,  2.4751e-03,  2.4460e-03,\n",
              "                        1.9345e-03, -3.4684e-03, -7.9477e-03,  6.2079e-03,  6.6678e-03,\n",
              "                        3.5781e-03,  8.7123e-03,  1.7533e-03,  3.4010e-03,  5.9777e-03,\n",
              "                       -7.8856e-03, -5.1612e-03,  6.7411e-03,  1.0994e-02,  4.7787e-03,\n",
              "                        3.2921e-03,  9.1656e-03,  1.3560e-03,  7.0880e-03,  6.4780e-03,\n",
              "                       -3.5377e-03,  8.1345e-03, -4.5393e-03, -3.4726e-03, -5.6400e-03,\n",
              "                       -5.4519e-03,  8.2557e-03,  1.5337e-03,  9.7327e-03,  6.1963e-03,\n",
              "                        8.8256e-03,  9.3832e-03, -8.4428e-03, -3.7721e-03, -1.8288e-03,\n",
              "                        1.0403e-02, -1.0355e-02,  5.2217e-03, -2.0061e-03, -6.2602e-03,\n",
              "                       -9.5476e-05,  6.1724e-03, -7.8473e-03,  5.4229e-03, -4.3577e-03,\n",
              "                        5.5142e-03, -3.2796e-03,  1.3079e-03, -7.6600e-03, -7.7008e-03,\n",
              "                       -9.0519e-03,  3.6916e-03,  8.2886e-04, -4.8266e-04, -7.0475e-03,\n",
              "                       -9.8792e-03, -6.7929e-03, -2.4956e-03, -3.3827e-03,  6.3309e-03,\n",
              "                        6.1973e-03,  6.0983e-03, -7.3339e-03,  8.3331e-05,  1.8361e-03,\n",
              "                        5.4275e-03,  1.0640e-02, -4.3196e-03,  6.6189e-03,  4.7652e-03,\n",
              "                        9.9756e-04, -5.5147e-03, -3.0770e-03,  5.6816e-03,  8.5766e-03,\n",
              "                       -6.6772e-03,  1.2639e-03, -8.2120e-03, -8.7945e-03, -1.8505e-03,\n",
              "                        9.4866e-03,  3.0225e-03,  9.1949e-03,  3.2566e-03, -4.5941e-03,\n",
              "                       -7.7843e-03,  6.0286e-03, -6.9483e-03,  2.7482e-03, -7.3948e-03,\n",
              "                        6.2860e-03, -4.8992e-03,  4.2554e-03,  3.3871e-03, -9.4632e-03,\n",
              "                       -6.3830e-03, -3.3265e-03, -4.3975e-03,  8.9130e-03, -5.6275e-04,\n",
              "                        3.7128e-03, -8.3674e-03, -1.0310e-02,  3.3743e-03, -8.8399e-03,\n",
              "                        6.5925e-03,  7.3868e-03,  6.0704e-03, -9.1660e-03, -3.3096e-03,\n",
              "                        6.3438e-03,  8.9259e-03, -5.4175e-03,  1.7854e-03,  6.7975e-03,\n",
              "                        6.1265e-03, -6.5741e-03,  5.8448e-03,  3.0257e-03, -6.7106e-04,\n",
              "                        3.7996e-03, -1.0774e-03, -1.8748e-03, -3.7294e-03, -2.0235e-03,\n",
              "                       -2.5069e-03, -6.0832e-03,  6.2051e-03,  4.4905e-03, -2.2730e-03,\n",
              "                        3.9221e-03,  2.3405e-03, -9.4231e-04, -9.7648e-04, -5.3345e-03,\n",
              "                       -5.9692e-04, -3.3741e-03,  2.1329e-03, -9.2209e-03, -9.0203e-04,\n",
              "                        6.6513e-03,  4.6466e-03,  4.5963e-03, -1.6836e-03,  5.7518e-03,\n",
              "                        5.4453e-03,  7.5723e-04, -1.5492e-03, -3.2662e-05, -7.2432e-03,\n",
              "                       -1.7222e-03,  9.8560e-03,  6.1800e-03,  2.2950e-03,  1.2232e-03,\n",
              "                       -3.4648e-03,  9.9650e-03, -4.8749e-03, -2.2308e-03, -7.0467e-03,\n",
              "                       -5.8004e-03,  3.6390e-03, -7.7843e-03, -1.7641e-03,  6.3329e-03,\n",
              "                        8.9389e-03,  6.7366e-03, -5.8310e-03, -8.8420e-04,  8.5654e-04,\n",
              "                       -3.8893e-03, -2.6870e-03,  5.9040e-03,  6.3937e-03,  8.8051e-03,\n",
              "                       -6.0526e-03, -1.3818e-03,  4.9880e-04,  2.4970e-03,  9.7006e-04,\n",
              "                        3.0888e-03, -8.4180e-03, -6.3999e-03, -5.4668e-03,  7.5140e-03,\n",
              "                        1.1632e-03,  1.0863e-02, -4.1625e-03,  8.2678e-04,  7.8006e-03,\n",
              "                       -3.0362e-04, -4.3857e-04,  1.8640e-03,  6.4618e-03,  5.3044e-03,\n",
              "                       -1.4936e-03, -2.7580e-04,  8.5333e-03, -1.9467e-03,  8.7842e-03,\n",
              "                       -6.6787e-03, -5.3916e-03, -6.3432e-03, -1.0405e-02, -2.0232e-03,\n",
              "                       -6.9507e-03, -3.2018e-03, -1.2804e-03, -1.9848e-04,  7.1640e-03,\n",
              "                       -5.7252e-03, -5.0713e-03, -4.5148e-03,  5.2529e-03,  1.0634e-02,\n",
              "                       -7.0448e-03,  3.8926e-03], requires_grad=True))),\n",
              "             ('model.2.scale', tensor(1.)),\n",
              "             ('model.2.zero_point', tensor(0)),\n",
              "             ('model.2._packed_params.dtype', torch.qint8),\n",
              "             ('model.2._packed_params._packed_params',\n",
              "              (tensor([[-0.0223,  0.0202,  0.0326,  ...,  0.0032,  0.0315, -0.0223],\n",
              "                       [-0.0110,  0.0067,  0.0039,  ..., -0.0255, -0.0333, -0.0439],\n",
              "                       [-0.0028,  0.0344, -0.0209,  ...,  0.0269,  0.0096,  0.0262],\n",
              "                       ...,\n",
              "                       [ 0.0319,  0.0067,  0.0142,  ..., -0.0206,  0.0014, -0.0408],\n",
              "                       [ 0.0025, -0.0269,  0.0064,  ..., -0.0209,  0.0365, -0.0202],\n",
              "                       [-0.0014, -0.0206, -0.0223,  ...,  0.0092, -0.0379, -0.0152]],\n",
              "                      size=(256, 512), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.00035441032378003,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-0.0439,  0.0052, -0.0086, -0.0230,  0.0143,  0.0141, -0.0116,  0.0374,\n",
              "                        0.0152, -0.0161, -0.0063, -0.0039,  0.0326,  0.0418, -0.0020, -0.0197,\n",
              "                       -0.0175, -0.0290,  0.0196, -0.0172, -0.0033, -0.0244, -0.0398,  0.0147,\n",
              "                        0.0127, -0.0378,  0.0257,  0.0127, -0.0103,  0.0186, -0.0085,  0.0160,\n",
              "                       -0.0377, -0.0022, -0.0278, -0.0381, -0.0083, -0.0213,  0.0089, -0.0034,\n",
              "                        0.0164,  0.0119,  0.0347, -0.0354, -0.0341, -0.0310, -0.0050,  0.0196,\n",
              "                        0.0230,  0.0206, -0.0416, -0.0246,  0.0295,  0.0053,  0.0071,  0.0354,\n",
              "                       -0.0094,  0.0090, -0.0308, -0.0381,  0.0305,  0.0159, -0.0261, -0.0165,\n",
              "                       -0.0158,  0.0234, -0.0074, -0.0202,  0.0359,  0.0240, -0.0134,  0.0342,\n",
              "                        0.0283, -0.0099, -0.0115,  0.0265,  0.0040,  0.0375,  0.0340,  0.0219,\n",
              "                       -0.0369, -0.0219, -0.0098,  0.0330,  0.0358, -0.0149,  0.0408,  0.0138,\n",
              "                        0.0275, -0.0438,  0.0427,  0.0285,  0.0232, -0.0219, -0.0409, -0.0237,\n",
              "                        0.0313,  0.0373,  0.0181,  0.0116, -0.0270, -0.0246, -0.0350,  0.0180,\n",
              "                       -0.0157, -0.0210, -0.0306,  0.0393,  0.0038,  0.0408, -0.0329,  0.0387,\n",
              "                       -0.0160, -0.0411, -0.0319, -0.0359,  0.0255, -0.0016,  0.0203,  0.0170,\n",
              "                       -0.0080, -0.0348,  0.0182,  0.0244, -0.0214,  0.0309, -0.0042, -0.0268,\n",
              "                       -0.0436, -0.0329, -0.0055, -0.0377, -0.0311, -0.0231, -0.0210, -0.0162,\n",
              "                       -0.0172,  0.0355, -0.0390, -0.0387,  0.0202, -0.0406,  0.0442, -0.0339,\n",
              "                        0.0034, -0.0328,  0.0370, -0.0214, -0.0402,  0.0371,  0.0194,  0.0368,\n",
              "                       -0.0390,  0.0414, -0.0087,  0.0106,  0.0219, -0.0046, -0.0319,  0.0290,\n",
              "                       -0.0105, -0.0333, -0.0378,  0.0090,  0.0271, -0.0362,  0.0157,  0.0421,\n",
              "                       -0.0313,  0.0098, -0.0319, -0.0388,  0.0019, -0.0027, -0.0109, -0.0142,\n",
              "                       -0.0209, -0.0071,  0.0307,  0.0324,  0.0104, -0.0092,  0.0107, -0.0240,\n",
              "                        0.0188,  0.0348, -0.0272,  0.0239, -0.0267,  0.0305, -0.0222, -0.0278,\n",
              "                       -0.0246, -0.0405,  0.0156, -0.0103, -0.0320, -0.0358, -0.0094,  0.0306,\n",
              "                       -0.0328,  0.0253, -0.0187, -0.0084, -0.0048,  0.0092,  0.0296,  0.0122,\n",
              "                        0.0390, -0.0043, -0.0058, -0.0419, -0.0111,  0.0309, -0.0043, -0.0032,\n",
              "                        0.0240,  0.0226, -0.0332, -0.0208, -0.0082, -0.0381,  0.0339,  0.0125,\n",
              "                       -0.0247,  0.0366,  0.0358, -0.0030,  0.0398,  0.0043, -0.0259, -0.0412,\n",
              "                       -0.0055,  0.0184,  0.0098, -0.0061, -0.0304, -0.0209, -0.0336, -0.0208,\n",
              "                        0.0431, -0.0123, -0.0346, -0.0130,  0.0086,  0.0312, -0.0350, -0.0353,\n",
              "                        0.0112,  0.0427, -0.0303,  0.0200,  0.0201,  0.0355,  0.0206, -0.0366],\n",
              "                      requires_grad=True))),\n",
              "             ('model.4.scale', tensor(1.)),\n",
              "             ('model.4.zero_point', tensor(0)),\n",
              "             ('model.4._packed_params.dtype', torch.qint8),\n",
              "             ('model.4._packed_params._packed_params',\n",
              "              (tensor([[-0.0528,  0.0314, -0.0577,  ..., -0.0249,  0.0488, -0.0194],\n",
              "                       [-0.0279,  0.0010,  0.0428,  ..., -0.0194,  0.0627,  0.0075],\n",
              "                       [ 0.0134, -0.0199,  0.0408,  ...,  0.0229,  0.0557,  0.0204],\n",
              "                       ...,\n",
              "                       [-0.0060, -0.0443,  0.0617,  ..., -0.0020,  0.0025, -0.0403],\n",
              "                       [-0.0209, -0.0319, -0.0353,  ...,  0.0144, -0.0179, -0.0592],\n",
              "                       [-0.0124, -0.0533, -0.0338,  ..., -0.0124,  0.0070,  0.0314]],\n",
              "                      size=(128, 256), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.000497723463922739,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-0.0109,  0.0403, -0.0146,  0.0250,  0.0158, -0.0059,  0.0012,  0.0172,\n",
              "                       -0.0430, -0.0376, -0.0060,  0.0471,  0.0558, -0.0373,  0.0233, -0.0497,\n",
              "                        0.0476,  0.0512,  0.0478,  0.0126, -0.0156, -0.0182, -0.0267, -0.0165,\n",
              "                        0.0050,  0.0528,  0.0499,  0.0176,  0.0403,  0.0349, -0.0136, -0.0537,\n",
              "                        0.0504, -0.0038,  0.0117,  0.0245, -0.0475, -0.0277, -0.0058, -0.0219,\n",
              "                       -0.0396,  0.0509, -0.0491, -0.0448,  0.0096, -0.0172,  0.0561,  0.0328,\n",
              "                       -0.0573, -0.0192,  0.0067, -0.0482, -0.0471, -0.0499, -0.0226, -0.0082,\n",
              "                        0.0585,  0.0089,  0.0526,  0.0413,  0.0383, -0.0107,  0.0237, -0.0227,\n",
              "                       -0.0234,  0.0201, -0.0088, -0.0573,  0.0029,  0.0237, -0.0138,  0.0288,\n",
              "                        0.0336,  0.0178, -0.0345,  0.0171,  0.0437,  0.0537, -0.0531,  0.0343,\n",
              "                        0.0468,  0.0189, -0.0455,  0.0478,  0.0255, -0.0088,  0.0040, -0.0325,\n",
              "                        0.0372,  0.0465,  0.0009,  0.0529, -0.0289, -0.0183, -0.0395,  0.0216,\n",
              "                        0.0541,  0.0035,  0.0611,  0.0393, -0.0036,  0.0518,  0.0613,  0.0019,\n",
              "                       -0.0298, -0.0566, -0.0154, -0.0169,  0.0183,  0.0118,  0.0488,  0.0043,\n",
              "                       -0.0579, -0.0248,  0.0114,  0.0154,  0.0430,  0.0132, -0.0367, -0.0393,\n",
              "                       -0.0205,  0.0613, -0.0351, -0.0310, -0.0305, -0.0454,  0.0424,  0.0016],\n",
              "                      requires_grad=True))),\n",
              "             ('model.6.scale', tensor(1.)),\n",
              "             ('model.6.zero_point', tensor(0)),\n",
              "             ('model.6._packed_params.dtype', torch.qint8),\n",
              "             ('model.6._packed_params._packed_params',\n",
              "              (tensor([[ 0.0091, -0.0056, -0.0876,  ...,  0.0070,  0.0743,  0.0161],\n",
              "                       [-0.0771, -0.0869, -0.0434,  ..., -0.0371,  0.0315,  0.0042],\n",
              "                       [ 0.0385, -0.0490, -0.0582,  ...,  0.0652,  0.0603, -0.0448],\n",
              "                       ...,\n",
              "                       [ 0.0217,  0.0112, -0.0876,  ..., -0.0708, -0.0715,  0.0350],\n",
              "                       [-0.0154,  0.0021,  0.0546,  ...,  0.0189,  0.0392, -0.0035],\n",
              "                       [-0.0680, -0.0666,  0.0434,  ...,  0.0224, -0.0778,  0.0448]],\n",
              "                      size=(64, 128), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.0007006059749983251,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-0.0482, -0.0485,  0.0675, -0.0445,  0.0564,  0.0664,  0.0333,  0.0358,\n",
              "                       -0.0288,  0.0576,  0.0534, -0.0360, -0.0061,  0.0117, -0.0729,  0.0278,\n",
              "                       -0.0781, -0.0365,  0.0113,  0.0220,  0.0188, -0.0362,  0.0660,  0.0245,\n",
              "                        0.0279,  0.0296,  0.0208,  0.0640,  0.0105, -0.0225,  0.0575, -0.0566,\n",
              "                       -0.0049, -0.0277,  0.0636, -0.0016,  0.0002, -0.0691,  0.0653,  0.0069,\n",
              "                        0.0185, -0.0136,  0.0556,  0.0661, -0.0306,  0.0140, -0.0367,  0.0547,\n",
              "                       -0.0048,  0.0543,  0.0572, -0.0512,  0.0644, -0.0068, -0.0605,  0.0509,\n",
              "                        0.0400, -0.0096, -0.0314,  0.0286,  0.0364, -0.0568, -0.0547,  0.0236],\n",
              "                      requires_grad=True))),\n",
              "             ('model.8.scale', tensor(1.)),\n",
              "             ('model.8.zero_point', tensor(0)),\n",
              "             ('model.8._packed_params.dtype', torch.qint8),\n",
              "             ('model.8._packed_params._packed_params',\n",
              "              (tensor([[-0.1133,  0.1103, -0.0345, -0.0660, -0.1074, -0.0236, -0.0256, -0.1034,\n",
              "                         0.0768, -0.0384, -0.0207, -0.0542, -0.0611, -0.0611, -0.0187,  0.1093,\n",
              "                         0.0837, -0.0719,  0.1162,  0.1192,  0.0394,  0.0956,  0.0227,  0.0010,\n",
              "                        -0.0483, -0.0049, -0.0473,  0.1034, -0.0512,  0.0828,  0.0502, -0.0030,\n",
              "                        -0.0138,  0.0887, -0.0010,  0.0118,  0.0847, -0.1054, -0.1103,  0.0896,\n",
              "                        -0.0542,  0.0680, -0.0059, -0.0305,  0.0118,  0.0916,  0.0581,  0.0591,\n",
              "                         0.1123, -0.0906,  0.0177, -0.0148, -0.0384, -0.1074, -0.0158, -0.0453,\n",
              "                         0.0847, -0.0946,  0.1103,  0.0650, -0.0276, -0.0709, -0.0424, -0.0414],\n",
              "                       [ 0.0000,  0.1172, -0.0749, -0.0906,  0.0443,  0.0630,  0.0364, -0.0443,\n",
              "                         0.0532,  0.0709, -0.0630,  0.0542,  0.1182,  0.0039, -0.0335,  0.0502,\n",
              "                         0.0138,  0.0630,  0.1251,  0.0128,  0.0867, -0.0335, -0.0483,  0.0729,\n",
              "                         0.1172, -0.0030, -0.0562,  0.1251,  0.1015, -0.0680,  0.1044,  0.0177,\n",
              "                         0.1241,  0.0965, -0.1123, -0.0768, -0.0463, -0.0384,  0.0532, -0.0906,\n",
              "                        -0.0601,  0.0660, -0.1162,  0.0276, -0.0227,  0.0532,  0.0207,  0.0305,\n",
              "                         0.0896,  0.0532, -0.0926, -0.0522, -0.1212, -0.0236,  0.0956, -0.0473,\n",
              "                        -0.1113, -0.0049,  0.0837, -0.0581,  0.0404,  0.0995, -0.0315,  0.1103]],\n",
              "                      size=(2, 64), dtype=torch.qint8,\n",
              "                      quantization_scheme=torch.per_tensor_affine, scale=0.0009851225186139345,\n",
              "                      zero_point=0),\n",
              "               Parameter containing:\n",
              "               tensor([-0.0984, -0.0943], requires_grad=True)))])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Half Precision"
      ],
      "metadata": {
        "id": "QuqczvxsYN40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Only safe if model supports it — works fine for MLPs\n",
        "half_model = copy.deepcopy(model).half()\n",
        "X_test_half = X_test.half()  # Also convert input\n",
        "half_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = half_model(X_test_half)\n",
        "    preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "half_accuracy = accuracy_score(y_test, preds)\n",
        "half_inference_time_ms = get_avg_inference_time(half_model, X_test_half)\n",
        "\n",
        "print(f\"[Half Precision] Accuracy: {half_accuracy:.4f}, Inference Time: {half_inference_time_ms:.2f} ms\")\n",
        "\n",
        "torch.save(half_model.state_dict(), \"half_precision.pt\")\n",
        "size_mb_half = os.path.getsize(\"half_precision.pt\") / (1024 * 1024)\n",
        "print(f\"[Half Precision] Model Size: {size_mb_half:.2f} MB\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "okA76SDbYNlR",
        "outputId": "278543e4-f3f8-498d-ae8f-4aa347bca0ab"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Half Precision] Accuracy: 0.4992, Inference Time: 2430.92 ms\n",
            "[Half Precision] Model Size: 10.10 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "half_model.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jNavIZ0fv55Q",
        "outputId": "0c2f1f72-beef-49d7-de6f-7fc668c7bfbf"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('model.0.weight',\n",
              "              tensor([[-0.0033,  0.0092, -0.0031,  ..., -0.0092, -0.0089,  0.0054],\n",
              "                      [-0.0049,  0.0019,  0.0071,  ..., -0.0097, -0.0079, -0.0015],\n",
              "                      [-0.0057,  0.0039, -0.0012,  ...,  0.0043, -0.0050, -0.0081],\n",
              "                      ...,\n",
              "                      [ 0.0067,  0.0021,  0.0080,  ...,  0.0022,  0.0064,  0.0103],\n",
              "                      [-0.0014, -0.0013, -0.0027,  ...,  0.0001,  0.0088,  0.0092],\n",
              "                      [-0.0100,  0.0066, -0.0061,  ...,  0.0035, -0.0028, -0.0003]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.0.bias',\n",
              "              tensor([-7.6561e-03, -4.7913e-03,  5.4169e-03, -3.4962e-03, -5.9662e-03,\n",
              "                       6.1836e-03,  4.4861e-03,  4.3869e-03,  5.6190e-03, -2.5368e-03,\n",
              "                       4.8065e-03, -3.8185e-03,  4.6768e-03, -2.1343e-03, -1.2636e-04,\n",
              "                       8.2321e-03,  4.2772e-04,  3.9005e-03, -7.9117e-03, -4.6806e-03,\n",
              "                      -6.4011e-03, -2.1815e-04,  3.4065e-03, -7.6447e-03, -1.3285e-03,\n",
              "                       4.9629e-03, -3.9024e-03, -3.9558e-03,  3.8681e-03, -5.8975e-03,\n",
              "                       7.8888e-03, -4.0054e-03, -6.8817e-03, -6.1378e-03,  9.9640e-03,\n",
              "                      -2.2430e-03,  3.3512e-03, -4.3945e-03,  3.7022e-03,  6.1302e-03,\n",
              "                      -8.3065e-04, -3.5725e-03,  7.1449e-03,  7.0305e-03,  5.5923e-03,\n",
              "                      -1.9684e-03,  6.8321e-03,  7.7248e-03,  1.4143e-03,  9.7733e-03,\n",
              "                      -3.1681e-03, -1.0315e-02,  1.0635e-02,  8.7585e-03, -6.2180e-03,\n",
              "                       1.6508e-03,  1.0155e-02, -6.5804e-03, -4.3809e-05,  6.0921e-03,\n",
              "                       3.7994e-03,  5.5923e-03, -5.6610e-03,  9.0027e-03,  1.0590e-02,\n",
              "                      -1.0834e-02, -9.0027e-03,  7.9880e-03, -1.8301e-03,  9.5215e-03,\n",
              "                      -7.7667e-03,  5.3310e-04,  1.4315e-03,  7.3967e-03,  7.0992e-03,\n",
              "                      -2.7733e-03,  5.6839e-03, -2.3975e-03, -7.4921e-03,  8.3847e-03,\n",
              "                       6.4926e-03,  7.9956e-03,  2.7313e-03,  5.0879e-04,  7.8430e-03,\n",
              "                       6.2561e-03, -9.7046e-03, -5.7869e-03, -3.7212e-03, -7.7248e-03,\n",
              "                       1.0269e-02,  6.0654e-03,  2.3293e-04, -5.0125e-03,  5.7411e-03,\n",
              "                      -8.4686e-03, -1.6184e-03, -2.9392e-03,  1.8549e-03,  5.7793e-03,\n",
              "                      -1.7405e-03, -8.8272e-03, -7.6752e-03, -7.4883e-03,  7.4997e-03,\n",
              "                       5.6696e-04,  1.5221e-03, -2.4509e-03,  4.2701e-04, -1.0460e-02,\n",
              "                      -8.1863e-03,  1.0635e-02, -2.2736e-03,  3.4904e-03, -5.0659e-03,\n",
              "                       2.0981e-04, -2.7514e-04, -7.9193e-03,  5.9509e-03, -2.1381e-03,\n",
              "                       2.1915e-03,  9.3985e-04, -8.0490e-03, -1.3599e-03, -4.4556e-03,\n",
              "                      -3.1090e-03, -3.2578e-03,  6.9160e-03, -2.3174e-03,  2.6512e-03,\n",
              "                       3.3550e-03,  3.5152e-03, -3.9749e-03,  7.1487e-03, -8.1635e-04,\n",
              "                      -7.9441e-04, -1.2436e-03, -9.8572e-03,  4.1237e-03,  7.3090e-03,\n",
              "                      -2.5749e-03, -7.3662e-03,  2.3327e-03,  7.5569e-03, -1.0384e-02,\n",
              "                       4.8943e-03,  2.0695e-03, -8.7509e-03,  2.7847e-03,  1.9855e-03,\n",
              "                       2.8667e-03, -4.1008e-03, -2.9678e-03, -5.0850e-03,  7.0190e-03,\n",
              "                      -7.7248e-03,  2.9583e-03,  2.8477e-03,  3.7346e-03,  2.1915e-03,\n",
              "                      -7.2432e-04, -4.7455e-03,  7.0381e-03,  1.0384e-02, -9.7942e-04,\n",
              "                       3.6120e-04, -7.5531e-04, -8.9111e-03, -8.8730e-03, -1.0658e-02,\n",
              "                      -6.7329e-03, -6.7291e-03, -1.6708e-03,  1.1139e-03,  9.6512e-03,\n",
              "                      -5.8823e-03,  7.8354e-03,  4.0169e-03,  3.2692e-03,  9.6893e-03,\n",
              "                      -7.5912e-03,  5.5275e-03, -3.5954e-03,  6.8665e-04, -2.9144e-03,\n",
              "                       5.1003e-03, -8.1635e-03,  1.0269e-02,  6.2752e-03,  6.4240e-03,\n",
              "                      -3.7880e-03,  6.6910e-03, -6.6376e-03,  6.9923e-03, -3.1414e-03,\n",
              "                      -9.0408e-03, -4.9477e-03,  7.5912e-03,  6.7940e-03, -3.5954e-03,\n",
              "                      -6.0425e-03,  2.4052e-03, -2.4128e-03, -5.4626e-03, -4.8599e-03,\n",
              "                      -4.7994e-04, -8.8453e-04,  1.6041e-03,  4.5624e-03, -5.8022e-03,\n",
              "                      -5.6381e-03,  7.7400e-03, -2.0485e-03,  6.3515e-03, -5.3635e-03,\n",
              "                       1.0567e-02,  1.4343e-03, -1.0509e-03, -5.3062e-03,  7.7705e-03,\n",
              "                       5.8975e-03,  2.3389e-04, -3.1967e-03, -6.2790e-03, -1.2703e-03,\n",
              "                      -4.1389e-03,  3.4313e-03,  2.7409e-03, -6.9618e-03, -5.3024e-03,\n",
              "                      -1.0941e-02,  1.2751e-03,  6.6223e-03, -5.7068e-03, -5.3215e-03,\n",
              "                      -6.2294e-03,  9.2926e-03, -5.9814e-03,  1.0017e-02, -5.9052e-03,\n",
              "                      -8.7585e-03, -3.9291e-03,  9.0408e-03,  8.7204e-03, -8.7070e-04,\n",
              "                       1.0315e-02, -7.1144e-04, -5.2261e-03, -4.2419e-03, -3.4962e-03,\n",
              "                       8.3923e-03,  6.3896e-03,  1.0162e-02,  9.9258e-03,  7.5951e-03,\n",
              "                       2.2049e-03,  3.4275e-03, -2.3842e-04,  3.2949e-04,  7.2479e-03,\n",
              "                       2.2507e-03, -8.9340e-03,  1.0216e-02,  8.1558e-03, -4.0817e-03,\n",
              "                       6.4373e-04,  3.4161e-03,  3.9482e-03, -6.4516e-04, -7.8678e-04,\n",
              "                      -4.8027e-03, -8.9264e-03, -9.6130e-03,  4.3030e-03,  2.8687e-03,\n",
              "                      -7.3929e-03, -7.5645e-03, -8.3694e-03,  6.7520e-03,  1.0803e-02,\n",
              "                      -7.1182e-03, -3.8123e-04, -3.1471e-03,  7.9880e-03,  8.4610e-03,\n",
              "                      -8.7891e-03, -8.1253e-03,  2.0242e-04,  7.7858e-03, -1.3471e-04,\n",
              "                       3.6964e-03, -4.3755e-03, -8.6288e-03,  2.4757e-03,  2.4452e-03,\n",
              "                       1.9341e-03, -3.4676e-03, -7.9498e-03,  6.2065e-03,  6.6681e-03,\n",
              "                       3.5782e-03,  8.7128e-03,  1.7529e-03,  3.4008e-03,  5.9776e-03,\n",
              "                      -7.8888e-03, -5.1613e-03,  6.7406e-03,  1.0994e-02,  4.7798e-03,\n",
              "                       3.2921e-03,  9.1629e-03,  1.3561e-03,  7.0877e-03,  6.4774e-03,\n",
              "                      -3.5381e-03,  8.1329e-03, -4.5395e-03, -3.4733e-03, -5.6419e-03,\n",
              "                      -5.4512e-03,  8.2550e-03,  1.5335e-03,  9.7351e-03,  6.1951e-03,\n",
              "                       8.8272e-03,  9.3842e-03, -8.4457e-03, -3.7727e-03, -1.8291e-03,\n",
              "                       1.0406e-02, -1.0353e-02,  5.2223e-03, -2.0065e-03, -6.2599e-03,\n",
              "                      -9.5487e-05,  6.1722e-03, -7.8506e-03,  5.4245e-03, -4.3564e-03,\n",
              "                       5.5161e-03, -3.2787e-03,  1.3075e-03, -7.6599e-03, -7.7019e-03,\n",
              "                      -9.0485e-03,  3.6907e-03,  8.2874e-04, -4.8256e-04, -7.0457e-03,\n",
              "                      -9.8801e-03, -6.7940e-03, -2.4948e-03, -3.3836e-03,  6.3324e-03,\n",
              "                       6.1989e-03,  6.0997e-03, -7.3357e-03,  8.3327e-05,  1.8358e-03,\n",
              "                       5.4283e-03,  1.0643e-02, -4.3182e-03,  6.6185e-03,  4.7646e-03,\n",
              "                       9.9754e-04, -5.5161e-03, -3.0766e-03,  5.6801e-03,  8.5754e-03,\n",
              "                      -6.6757e-03,  1.2636e-03, -8.2092e-03, -8.7967e-03, -1.8501e-03,\n",
              "                       9.4833e-03,  3.0231e-03,  9.1934e-03,  3.2558e-03, -4.5929e-03,\n",
              "                      -7.7858e-03,  6.0272e-03, -6.9466e-03,  2.7485e-03, -7.3929e-03,\n",
              "                       6.2866e-03, -4.8981e-03,  4.2572e-03,  3.3875e-03, -9.4604e-03,\n",
              "                      -6.3820e-03, -3.3264e-03, -4.3983e-03,  8.9111e-03, -5.6267e-04,\n",
              "                       3.7136e-03, -8.3694e-03, -1.0307e-02,  3.3741e-03, -8.8425e-03,\n",
              "                       6.5918e-03,  7.3853e-03,  6.0692e-03, -9.1629e-03, -3.3092e-03,\n",
              "                       6.3438e-03,  8.9264e-03, -5.4169e-03,  1.7853e-03,  6.7978e-03,\n",
              "                       6.1264e-03, -6.5727e-03,  5.8441e-03,  3.0251e-03, -6.7091e-04,\n",
              "                       3.7994e-03, -1.0777e-03, -1.8749e-03, -3.7289e-03, -2.0237e-03,\n",
              "                      -2.5063e-03, -6.0844e-03,  6.2065e-03,  4.4899e-03, -2.2736e-03,\n",
              "                       3.9215e-03,  2.3403e-03, -9.4223e-04, -9.7656e-04, -5.3329e-03,\n",
              "                      -5.9700e-04, -3.3741e-03,  2.1324e-03, -9.2239e-03, -9.0218e-04,\n",
              "                       6.6528e-03,  4.6463e-03,  4.5967e-03, -1.6832e-03,  5.7526e-03,\n",
              "                       5.4436e-03,  7.5722e-04, -1.5488e-03, -3.2663e-05, -7.2441e-03,\n",
              "                      -1.7223e-03,  9.8572e-03,  6.1798e-03,  2.2945e-03,  1.2236e-03,\n",
              "                      -3.4657e-03,  9.9640e-03, -4.8752e-03, -2.2316e-03, -7.0457e-03,\n",
              "                      -5.8022e-03,  3.6392e-03, -7.7858e-03, -1.7643e-03,  6.3324e-03,\n",
              "                       8.9417e-03,  6.7368e-03, -5.8327e-03, -8.8406e-04,  8.5640e-04,\n",
              "                      -3.8891e-03, -2.6875e-03,  5.9052e-03,  6.3934e-03,  8.8043e-03,\n",
              "                      -6.0539e-03, -1.3819e-03,  4.9877e-04,  2.4967e-03,  9.6989e-04,\n",
              "                       3.0880e-03, -8.4152e-03, -6.4011e-03, -5.4665e-03,  7.5150e-03,\n",
              "                       1.1635e-03,  1.0864e-02, -4.1618e-03,  8.2684e-04,  7.8011e-03,\n",
              "                      -3.0351e-04, -4.3869e-04,  1.8644e-03,  6.4621e-03,  5.3062e-03,\n",
              "                      -1.4935e-03, -2.7585e-04,  8.5297e-03, -1.9464e-03,  8.7814e-03,\n",
              "                      -6.6795e-03, -5.3902e-03, -6.3438e-03, -1.0406e-02, -2.0237e-03,\n",
              "                      -6.9504e-03, -3.2024e-03, -1.2808e-03, -1.9848e-04,  7.1640e-03,\n",
              "                      -5.7259e-03, -5.0697e-03, -4.5166e-03,  5.2528e-03,  1.0635e-02,\n",
              "                      -7.0457e-03,  3.8929e-03], dtype=torch.float16)),\n",
              "             ('model.2.weight',\n",
              "              tensor([[-0.0223,  0.0201,  0.0326,  ...,  0.0032,  0.0315, -0.0224],\n",
              "                      [-0.0111,  0.0067,  0.0038,  ..., -0.0255, -0.0333, -0.0439],\n",
              "                      [-0.0028,  0.0343, -0.0208,  ...,  0.0271,  0.0096,  0.0262],\n",
              "                      ...,\n",
              "                      [ 0.0320,  0.0066,  0.0143,  ..., -0.0206,  0.0014, -0.0406],\n",
              "                      [ 0.0026, -0.0271,  0.0064,  ..., -0.0210,  0.0363, -0.0201],\n",
              "                      [-0.0013, -0.0205, -0.0225,  ...,  0.0091, -0.0380, -0.0153]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.2.bias',\n",
              "              tensor([-0.0439,  0.0052, -0.0086, -0.0230,  0.0143,  0.0141, -0.0116,  0.0374,\n",
              "                       0.0152, -0.0161, -0.0063, -0.0039,  0.0326,  0.0418, -0.0020, -0.0197,\n",
              "                      -0.0175, -0.0290,  0.0196, -0.0172, -0.0033, -0.0244, -0.0398,  0.0147,\n",
              "                       0.0127, -0.0378,  0.0257,  0.0127, -0.0103,  0.0186, -0.0085,  0.0160,\n",
              "                      -0.0378, -0.0022, -0.0278, -0.0381, -0.0083, -0.0213,  0.0089, -0.0034,\n",
              "                       0.0164,  0.0119,  0.0347, -0.0354, -0.0341, -0.0310, -0.0050,  0.0196,\n",
              "                       0.0230,  0.0206, -0.0416, -0.0246,  0.0295,  0.0052,  0.0071,  0.0354,\n",
              "                      -0.0094,  0.0090, -0.0308, -0.0381,  0.0305,  0.0159, -0.0261, -0.0165,\n",
              "                      -0.0158,  0.0234, -0.0074, -0.0202,  0.0359,  0.0240, -0.0134,  0.0342,\n",
              "                       0.0283, -0.0099, -0.0115,  0.0265,  0.0040,  0.0374,  0.0340,  0.0219,\n",
              "                      -0.0369, -0.0219, -0.0098,  0.0330,  0.0358, -0.0149,  0.0408,  0.0138,\n",
              "                       0.0275, -0.0438,  0.0427,  0.0285,  0.0231, -0.0219, -0.0409, -0.0237,\n",
              "                       0.0313,  0.0373,  0.0181,  0.0116, -0.0270, -0.0246, -0.0350,  0.0180,\n",
              "                      -0.0157, -0.0210, -0.0305,  0.0393,  0.0038,  0.0408, -0.0329,  0.0388,\n",
              "                      -0.0160, -0.0411, -0.0319, -0.0359,  0.0255, -0.0016,  0.0203,  0.0170,\n",
              "                      -0.0080, -0.0348,  0.0182,  0.0244, -0.0214,  0.0309, -0.0042, -0.0268,\n",
              "                      -0.0436, -0.0329, -0.0055, -0.0377, -0.0311, -0.0231, -0.0210, -0.0162,\n",
              "                      -0.0172,  0.0355, -0.0390, -0.0387,  0.0202, -0.0406,  0.0442, -0.0339,\n",
              "                       0.0034, -0.0328,  0.0370, -0.0214, -0.0402,  0.0371,  0.0194,  0.0368,\n",
              "                      -0.0390,  0.0414, -0.0087,  0.0106,  0.0219, -0.0046, -0.0319,  0.0290,\n",
              "                      -0.0105, -0.0333, -0.0378,  0.0090,  0.0271, -0.0362,  0.0157,  0.0421,\n",
              "                      -0.0312,  0.0098, -0.0319, -0.0388,  0.0019, -0.0027, -0.0109, -0.0142,\n",
              "                      -0.0209, -0.0071,  0.0307,  0.0324,  0.0104, -0.0092,  0.0107, -0.0240,\n",
              "                       0.0188,  0.0348, -0.0272,  0.0239, -0.0267,  0.0305, -0.0222, -0.0278,\n",
              "                      -0.0246, -0.0405,  0.0156, -0.0103, -0.0320, -0.0358, -0.0094,  0.0306,\n",
              "                      -0.0328,  0.0253, -0.0187, -0.0084, -0.0048,  0.0092,  0.0296,  0.0122,\n",
              "                       0.0390, -0.0043, -0.0058, -0.0419, -0.0111,  0.0309, -0.0043, -0.0032,\n",
              "                       0.0240,  0.0226, -0.0332, -0.0208, -0.0082, -0.0381,  0.0339,  0.0125,\n",
              "                      -0.0247,  0.0366,  0.0358, -0.0030,  0.0398,  0.0043, -0.0259, -0.0411,\n",
              "                      -0.0055,  0.0184,  0.0098, -0.0061, -0.0304, -0.0209, -0.0336, -0.0208,\n",
              "                       0.0431, -0.0123, -0.0346, -0.0130,  0.0086,  0.0312, -0.0350, -0.0353,\n",
              "                       0.0112,  0.0427, -0.0303,  0.0200,  0.0201,  0.0356,  0.0206, -0.0366],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.4.weight',\n",
              "              tensor([[-0.0526,  0.0314, -0.0576,  ..., -0.0249,  0.0487, -0.0196],\n",
              "                      [-0.0280,  0.0009,  0.0427,  ..., -0.0193,  0.0629,  0.0077],\n",
              "                      [ 0.0137, -0.0197,  0.0406,  ...,  0.0229,  0.0557,  0.0202],\n",
              "                      ...,\n",
              "                      [-0.0061, -0.0441,  0.0616,  ..., -0.0018,  0.0025, -0.0403],\n",
              "                      [-0.0208, -0.0319, -0.0354,  ...,  0.0145, -0.0177, -0.0591],\n",
              "                      [-0.0124, -0.0532, -0.0340,  ..., -0.0123,  0.0069,  0.0314]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.4.bias',\n",
              "              tensor([-0.0109,  0.0403, -0.0146,  0.0249,  0.0158, -0.0059,  0.0012,  0.0172,\n",
              "                      -0.0430, -0.0376, -0.0060,  0.0471,  0.0558, -0.0373,  0.0233, -0.0497,\n",
              "                       0.0475,  0.0512,  0.0478,  0.0126, -0.0156, -0.0182, -0.0267, -0.0165,\n",
              "                       0.0050,  0.0528,  0.0499,  0.0176,  0.0403,  0.0349, -0.0136, -0.0537,\n",
              "                       0.0504, -0.0038,  0.0117,  0.0245, -0.0475, -0.0278, -0.0058, -0.0219,\n",
              "                      -0.0396,  0.0509, -0.0491, -0.0448,  0.0096, -0.0172,  0.0561,  0.0328,\n",
              "                      -0.0573, -0.0192,  0.0067, -0.0482, -0.0471, -0.0499, -0.0226, -0.0082,\n",
              "                       0.0585,  0.0089,  0.0526,  0.0413,  0.0383, -0.0107,  0.0237, -0.0227,\n",
              "                      -0.0234,  0.0201, -0.0088, -0.0573,  0.0029,  0.0237, -0.0138,  0.0288,\n",
              "                       0.0336,  0.0178, -0.0345,  0.0171,  0.0437,  0.0537, -0.0531,  0.0342,\n",
              "                       0.0468,  0.0189, -0.0455,  0.0478,  0.0255, -0.0088,  0.0040, -0.0325,\n",
              "                       0.0371,  0.0464,  0.0009,  0.0529, -0.0289, -0.0183, -0.0395,  0.0216,\n",
              "                       0.0541,  0.0035,  0.0611,  0.0392, -0.0036,  0.0518,  0.0613,  0.0019,\n",
              "                      -0.0298, -0.0566, -0.0154, -0.0169,  0.0183,  0.0118,  0.0488,  0.0043,\n",
              "                      -0.0579, -0.0247,  0.0114,  0.0154,  0.0430,  0.0132, -0.0367, -0.0393,\n",
              "                      -0.0205,  0.0613, -0.0351, -0.0310, -0.0305, -0.0453,  0.0424,  0.0016],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.6.weight',\n",
              "              tensor([[ 0.0095, -0.0058, -0.0875,  ...,  0.0073,  0.0746,  0.0162],\n",
              "                      [-0.0773, -0.0870, -0.0435,  ..., -0.0373,  0.0312,  0.0039],\n",
              "                      [ 0.0383, -0.0494, -0.0582,  ...,  0.0650,  0.0605, -0.0449],\n",
              "                      ...,\n",
              "                      [ 0.0215,  0.0115, -0.0876,  ..., -0.0709, -0.0717,  0.0351],\n",
              "                      [-0.0153,  0.0020,  0.0548,  ...,  0.0190,  0.0392, -0.0033],\n",
              "                      [-0.0682, -0.0664,  0.0436,  ...,  0.0221, -0.0776,  0.0451]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.6.bias',\n",
              "              tensor([-0.0482, -0.0485,  0.0675, -0.0445,  0.0564,  0.0664,  0.0333,  0.0358,\n",
              "                      -0.0288,  0.0576,  0.0534, -0.0360, -0.0061,  0.0117, -0.0729,  0.0278,\n",
              "                      -0.0781, -0.0365,  0.0113,  0.0220,  0.0188, -0.0362,  0.0660,  0.0245,\n",
              "                       0.0279,  0.0296,  0.0208,  0.0640,  0.0105, -0.0225,  0.0575, -0.0566,\n",
              "                      -0.0049, -0.0277,  0.0636, -0.0016,  0.0002, -0.0692,  0.0653,  0.0069,\n",
              "                       0.0185, -0.0136,  0.0556,  0.0661, -0.0306,  0.0140, -0.0367,  0.0547,\n",
              "                      -0.0048,  0.0543,  0.0573, -0.0512,  0.0644, -0.0068, -0.0605,  0.0509,\n",
              "                       0.0400, -0.0096, -0.0314,  0.0286,  0.0364, -0.0568, -0.0547,  0.0235],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.8.weight',\n",
              "              tensor([[-1.1377e-01,  1.1035e-01, -3.4607e-02, -6.6040e-02, -1.0687e-01,\n",
              "                       -2.3941e-02, -2.5238e-02, -1.0333e-01,  7.6843e-02, -3.8391e-02,\n",
              "                       -2.0889e-02, -5.4108e-02, -6.0883e-02, -6.1249e-02, -1.8387e-02,\n",
              "                        1.0950e-01,  8.4106e-02, -7.1838e-02,  1.1658e-01,  1.1963e-01,\n",
              "                        3.9551e-02,  9.5337e-02,  2.2964e-02,  6.4659e-04, -4.7852e-02,\n",
              "                       -5.1956e-03, -4.7302e-02,  1.0297e-01, -5.1361e-02,  8.2764e-02,\n",
              "                        4.9927e-02, -2.6588e-03, -1.3832e-02,  8.9050e-02, -1.4029e-03,\n",
              "                        1.2054e-02,  8.4534e-02, -1.0535e-01, -1.1023e-01,  9.0088e-02,\n",
              "                       -5.3894e-02,  6.7505e-02, -6.1150e-03, -3.0594e-02,  1.1337e-02,\n",
              "                        9.2041e-02,  5.8014e-02,  5.9204e-02,  1.1182e-01, -9.1003e-02,\n",
              "                        1.7838e-02, -1.5167e-02, -3.8391e-02, -1.0712e-01, -1.5404e-02,\n",
              "                       -4.4952e-02,  8.4595e-02, -9.4421e-02,  1.1066e-01,  6.4697e-02,\n",
              "                       -2.7908e-02, -7.0740e-02, -4.2084e-02, -4.0924e-02],\n",
              "                      [ 1.1671e-04,  1.1725e-01, -7.4402e-02, -9.0698e-02,  4.4159e-02,\n",
              "                        6.3416e-02,  3.6316e-02, -4.3884e-02,  5.3070e-02,  7.0862e-02,\n",
              "                       -6.2622e-02,  5.4352e-02,  1.1804e-01,  3.7956e-03, -3.3386e-02,\n",
              "                        5.0507e-02,  1.4122e-02,  6.3416e-02,  1.2500e-01,  1.2978e-02,\n",
              "                        8.6975e-02, -3.3020e-02, -4.8187e-02,  7.3120e-02,  1.1713e-01,\n",
              "                       -2.8725e-03, -5.6488e-02,  1.2561e-01,  1.0150e-01, -6.7505e-02,\n",
              "                        1.0443e-01,  1.7609e-02,  1.2421e-01,  9.6436e-02, -1.1261e-01,\n",
              "                       -7.6660e-02, -4.6783e-02, -3.8239e-02,  5.3314e-02, -9.0820e-02,\n",
              "                       -6.0364e-02,  6.5979e-02, -1.1621e-01,  2.7725e-02, -2.2354e-02,\n",
              "                        5.3070e-02,  2.0706e-02,  3.0869e-02,  8.9172e-02,  5.3162e-02,\n",
              "                       -9.2285e-02, -5.2429e-02, -1.2115e-01, -2.3270e-02,  9.5886e-02,\n",
              "                       -4.7638e-02, -1.1139e-01, -5.1041e-03,  8.3618e-02, -5.7861e-02,\n",
              "                        4.0588e-02,  9.9548e-02, -3.1067e-02,  1.1060e-01]],\n",
              "                     dtype=torch.float16)),\n",
              "             ('model.8.bias',\n",
              "              tensor([-0.0984, -0.0943], dtype=torch.float16))])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fill the table for different quantization techniques."
      ],
      "metadata": {
        "id": "rYrche4TuRTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Final Table\n",
        "data = {\n",
        "    \"Model Name\": [\"Original\", \"Dynamic\", \"Half\"],\n",
        "    \"Accuracy (Out of 100)\": [round(orig_acc * 100, 2), round(quantized_accuracy * 100, 2), round(half_accuracy * 100, 2)],\n",
        "    \"Storage (In MB)\": [round(orig_size, 2), round(size_mb, 2), round(size_mb_half, 2)],\n",
        "    \"Inference time (In ms)\": [round(orig_time, 2), round(inference_time_ms, 2), round(half_inference_time_ms, 2)],\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(data)\n",
        "print(\"\\n Final Quantization Comparison Table:\")\n",
        "print(results_df.to_markdown(index=False))\n"
      ],
      "metadata": {
        "id": "Pn_jj55TY06X",
        "outputId": "125c089f-3832-4383-8318-9e2e1cbea894",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Final Quantization Comparison Table:\n",
            "| Model Name   |   Accuracy (Out of 100) |   Storage (In MB) |   Inference time (In ms) |\n",
            "|:-------------|------------------------:|------------------:|-------------------------:|\n",
            "| Original     |                   49.92 |             20.2  |                   313.35 |\n",
            "| Dynamic      |                   49.92 |              5.06 |                   214.24 |\n",
            "| Half         |                   49.92 |             10.1  |                  2430.92 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Half Precision With GPU"
      ],
      "metadata": {
        "id": "qg98qdy9h_Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "half_model_gpu = copy.deepcopy(model).half().to(device)\n",
        "X_test_half_gpu = X_test.half().to(device)\n",
        "\n",
        "\n",
        "half_model_gpu.eval()\n",
        "with torch.no_grad():\n",
        "    outputs_gpu = half_model_gpu(X_test_half_gpu)\n",
        "    preds_gpu = torch.argmax(outputs_gpu, dim=1)\n",
        "\n",
        "half_accuracy_gpu = accuracy_score(y_test.cpu(), preds_gpu.cpu())\n",
        "\n",
        "# Measure average inference time over multiple runs\n",
        "def get_avg_inference_time_gpu(model, X_test, num_runs=10):\n",
        "    times = []\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_runs):\n",
        "            start = time.time()\n",
        "            _ = model(X_test)\n",
        "            if device.type == \"cuda\":\n",
        "                torch.cuda.synchronize()  # wait for GPU to finish\n",
        "            end = time.time()\n",
        "            times.append((end - start) * 1000)  # ms\n",
        "    return sum(times) / len(times)\n",
        "\n",
        "avg_time_ms = get_avg_inference_time_gpu(half_model_gpu, X_test_half_gpu)\n",
        "\n",
        "# Save and get model size\n",
        "torch.save(half_model_gpu.state_dict(), \"half_precision_gpu.pt\")\n",
        "size_mb_half_gpu = os.path.getsize(\"half_precision_gpu.pt\") / (1024 * 1024)\n",
        "\n",
        "# Final Output\n",
        "print(f\"[Half Precision - {device.type.upper()}] Accuracy: {half_accuracy_gpu:.4f}, Inference Time (avg): {avg_time_ms:.2f} ms\")\n",
        "print(f\"[Half Precision GPU] Model Size: {size_mb_half_gpu:.2f} MB\")\n"
      ],
      "metadata": {
        "id": "4mzX2K1dx4Xh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "876f1fff-9d61-4193-9edd-ef769c0ce507"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Half Precision - CUDA] Accuracy: 0.4992, Inference Time (avg): 1.36 ms\n",
            "[Half Precision GPU] Model Size: 10.10 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Final Table\n",
        "data = {\n",
        "    \"Model Name\": [\"Original\", \"Dynamic\", \"Half\", \"Half( WITH GPU)\"],\n",
        "    \"Accuracy (Out of 100)\": [round(orig_acc * 100, 2), round(quantized_accuracy * 100, 2), round(half_accuracy * 100, 2),round(half_accuracy_gpu * 100, 2)],\n",
        "    \"Storage (In MB)\": [round(orig_size, 2), round(size_mb, 2), round(size_mb_half, 2),round(size_mb_half_gpu, 2)],\n",
        "    \"Inference time (In ms)\": [round(orig_time, 2), round(inference_time_ms, 2), round(half_inference_time_ms, 2),round(avg_time_ms, 2)],\n",
        "}\n",
        "\n",
        "results_df = pd.DataFrame(data)\n",
        "print(\"\\n Final Quantization Comparison Table:\")\n",
        "print(results_df.to_markdown(index=False))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zVfNowWRgnq2",
        "outputId": "1797716d-654a-48ad-9d62-b705abf2b943"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Final Quantization Comparison Table:\n",
            "| Model Name      |   Accuracy (Out of 100) |   Storage (In MB) |   Inference time (In ms) |\n",
            "|:----------------|------------------------:|------------------:|-------------------------:|\n",
            "| Original        |                   49.92 |             20.2  |                   313.35 |\n",
            "| Dynamic         |                   49.92 |              5.06 |                   214.24 |\n",
            "| Half            |                   49.92 |             10.1  |                  2430.92 |\n",
            "| Half( WITH GPU) |                   49.92 |             10.1  |                     1.36 |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Half-precision (FP16) models are slower on CPU because most CPUs lack native support for FP16 arithmetic and optimized instructions to handle it efficiently. As a result, CPUs often emulate FP16 operations using slower FP32 computations, which introduces overhead and significantly increases inference time. This makes FP16 unsuitable for CPU inference, even though it performs exceptionally well on GPUs that are designed to accelerate half-precision operations."
      ],
      "metadata": {
        "id": "GbduF5jRS9Xq"
      }
    }
  ]
}